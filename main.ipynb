{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aae605",
   "metadata": {},
   "source": [
    "# AlayaCare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459a1b7",
   "metadata": {},
   "source": [
    "## Ingestion pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add70cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01683dc90e3e49f09af8c39f8c6161ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running pipeline:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a0a93960da418fa62e1bbabb9f888f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53f4fb3905247c4b86989a817d35972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing commute distance:   0%|          | 0/6902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ee982ce352426b910d471945b67a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing employee tenure:   0%|          | 0/6902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mSUCCESS: \u001b[0m Client_Data_Cleaning_Calculated_fields_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Clock_Data_Cleaning_Calculated_fields_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_Data_Cleaning_Calculated_fields_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Visit_Data_Cleaning_Calculated_fields_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Augmented_Visit_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Segmentation_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_History_Aggregation_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_History_Fill_Gaps_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_History_Calculated_Fields_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_History_Rolling_Features_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_History_Fill_Na_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Employee_History_Anomaly_Detection_Stage's Data integrity test has passed.\n",
      "\u001b[38;5;2mSUCCESS: \u001b[0m Y_Labels_Generation_Stage's Data integrity test has passed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_id</th>\n",
       "      <th>error</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>CLIENT_AGE is greater than 125</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>CLIENT_AGE is less than 0</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>DAY_HOURS is greater than 336</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>END_TIME is after 2023-09-01 00:00:00</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>NIGHT_HOURS is greater than 336</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>START_TIME is after 2023-09-01 00:00:00</td>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>START_TIME is before 2019-01-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>WEEKDAY_HOURS is greater than 336</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>EMPLOYEE_AGE is greater than 90</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>EMPLOYEE_AGE is less than 16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INVALID_VALUE</td>\n",
       "      <td>EMPLOYEE_ID is equal to 2</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>VISIT_END_AT_UTC is after 2023-09-01 00:00:00+...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>VISIT_HOURS_APPROVED is greater than 50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>VISIT_START_AT is after 2023-09-01 00:00:00</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>VISIT_UNIT_QTY is greater than 20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INVALID_VALUE</td>\n",
       "      <td>VISIT_HOURS_APPROVED - (DAY_HOURS + NIGHT_HOURS )</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUT_OF_RANGE</td>\n",
       "      <td>STATUS_START_DATE is after 2023-09-01 00:00:00</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        error_id                                              error count\n",
       "0   OUT_OF_RANGE                     CLIENT_AGE is greater than 125    20\n",
       "1   OUT_OF_RANGE                          CLIENT_AGE is less than 0   135\n",
       "0   OUT_OF_RANGE                      DAY_HOURS is greater than 336   409\n",
       "1   OUT_OF_RANGE              END_TIME is after 2023-09-01 00:00:00   393\n",
       "2   OUT_OF_RANGE                    NIGHT_HOURS is greater than 336   163\n",
       "3   OUT_OF_RANGE            START_TIME is after 2023-09-01 00:00:00  1361\n",
       "4   OUT_OF_RANGE           START_TIME is before 2019-01-01 00:00:00     2\n",
       "5   OUT_OF_RANGE                  WEEKDAY_HOURS is greater than 336    94\n",
       "0   OUT_OF_RANGE                    EMPLOYEE_AGE is greater than 90   528\n",
       "1   OUT_OF_RANGE                       EMPLOYEE_AGE is less than 16     1\n",
       "0  INVALID_VALUE                          EMPLOYEE_ID is equal to 2   704\n",
       "1   OUT_OF_RANGE  VISIT_END_AT_UTC is after 2023-09-01 00:00:00+...     2\n",
       "2   OUT_OF_RANGE            VISIT_HOURS_APPROVED is greater than 50     3\n",
       "3   OUT_OF_RANGE        VISIT_START_AT is after 2023-09-01 00:00:00   630\n",
       "4   OUT_OF_RANGE                  VISIT_UNIT_QTY is greater than 20     4\n",
       "0  INVALID_VALUE  VISIT_HOURS_APPROVED - (DAY_HOURS + NIGHT_HOURS )   353\n",
       "0   OUT_OF_RANGE     STATUS_START_DATE is after 2023-09-01 00:00:00    20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import path\n",
    "\n",
    "path.append(\"src\")\n",
    "from warnings import filterwarnings\n",
    "from datetime import datetime\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "from src.utility.environment import Environment\n",
    "from src.data.ingestion_pipeline.ingestion_pipeline import IngestionPipeline\n",
    "from src.data.ingestion_pipeline.stages.cleaning.client_data_cleaning_calculated_fields_stage import (\n",
    "    build_client_data_cleaning_calculated_fields_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.cleaning.clock_data_cleaning_calculated_fields_stage import (\n",
    "    build_clock_data_cleaning_calculated_fields_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.cleaning.employee_data_cleaning_calculated_fields_stage import (\n",
    "    build_employee_data_cleaning_calculated_fields_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.cleaning.visit_data_cleaning_calculated_fields_stage import (\n",
    "    build_visit_data_cleaning_calculated_fields_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.augmented_visit_stage import (\n",
    "    build_augmented_visit_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.segmentation_stage import (\n",
    "    build_segmentation_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.employee_history_aggregation_stage import (\n",
    "    build_employee_history_aggregation_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.employee_history_fill_gaps_stage import (\n",
    "    build_employee_history_fill_gaps_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.employee_history_calculated_fields_stage import (\n",
    "    build_employee_history_calculated_fields_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.employee_rolling_features_stage import (\n",
    "    build_employee_history_rolling_features_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.employee_history_fill_na_stage import (\n",
    "    build_employee_history_fill_na_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.employee_history_anomaly_detection_stage import (\n",
    "    build_employee_history_anomaly_detection_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.training_employee_history_stage import (\n",
    "    build_training_employee_history_stage,\n",
    ")\n",
    "from src.data.ingestion_pipeline.stages.y_labels_generation_stage import (\n",
    "    build_y_labels_generation_stage,\n",
    ")\n",
    "from src.utility.configs.configs import Configs\n",
    "\n",
    "# 6 Hours\n",
    "MAX_RUNTIME = 21600\n",
    "\n",
    "env = Environment()\n",
    "conf = Configs.EXPLAINABLE_BOOSTING_MACHINE_CONFIG\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "ingestion_pipeline = (\n",
    "    IngestionPipeline(\n",
    "        config=conf,\n",
    "        environment=env,\n",
    "        use_caching=True,\n",
    "        limit_dataframe_size=conf.limit_dataframe_size,\n",
    "        stages=[\n",
    "            build_client_data_cleaning_calculated_fields_stage(conf, env),\n",
    "            build_clock_data_cleaning_calculated_fields_stage(conf, env),\n",
    "            build_employee_data_cleaning_calculated_fields_stage(conf, env),\n",
    "            build_visit_data_cleaning_calculated_fields_stage(conf, env),\n",
    "            build_augmented_visit_stage(conf, env),\n",
    "            build_segmentation_stage(conf, env),\n",
    "            build_employee_history_aggregation_stage(conf, env),\n",
    "            build_employee_history_fill_gaps_stage(conf, env),\n",
    "            build_employee_history_calculated_fields_stage(conf, env),\n",
    "            build_employee_history_rolling_features_stage(conf, env),\n",
    "            build_employee_history_fill_na_stage(conf, env),\n",
    "            build_employee_history_anomaly_detection_stage(conf, env),\n",
    "            build_y_labels_generation_stage(conf, env),\n",
    "            build_training_employee_history_stage(conf, env),\n",
    "        ],\n",
    "    )\n",
    "    .build_pipeline()\n",
    "    .run_pipeline()\n",
    ")\n",
    "\n",
    "elapsed_time = (datetime.now() - start)\n",
    "\n",
    "if conf.is_test_run:\n",
    "    assert elapsed_time.total_seconds() < MAX_RUNTIME\n",
    "\n",
    "ingestion_pipeline.data_integrity_test(threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024dc3e",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utility.segmentation import DataSegmenter\n",
    "from src.data.ingestion_pipeline.ingestion_pipeline_stages import IngestionPipelineStages\n",
    "from src.data.schema.segmentation_schema import SegmentationSchema\n",
    "import json\n",
    "\n",
    "segmentation_columns = [\n",
    "    SegmentationSchema.AVG_VISIT_DURATION,\n",
    "    SegmentationSchema.AVG_WORK_HOURS_DEVIATION,\n",
    "    SegmentationSchema.AVG_VISIT_HOURS_PER_WEEK,\n",
    "    SegmentationSchema.AVG_VISITS_PER_WEEK,\n",
    "    SegmentationSchema.AVG_DISTANCE_TO_PATIENT,\n",
    "    SegmentationSchema.AVG_CODED_DIAGNOSIS_PER_PATIENT,\n",
    "    SegmentationSchema.AVG_WAS_LATE_BY,\n",
    "    SegmentationSchema.AVG_LATE_ARRIVALS_PER_WEEK,\n",
    "    SegmentationSchema.UNIQUE_CLIENTS_RATIO,\n",
    "]\n",
    "\n",
    "demographic_columns = [\n",
    "    SegmentationSchema.EMPLOYEE_AGE,\n",
    "    SegmentationSchema.EMPLOYEE_GENDER,\n",
    "    SegmentationSchema.EMPLOYEE_TENURE,\n",
    "    SegmentationSchema.EMPLOYEE_JOB_TITLE,\n",
    "    SegmentationSchema.AVG_HOURLY_PAY,\n",
    "    SegmentationSchema.COMPLETED_ADL_RATIO\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    SegmentationSchema.EMPLOYEE_GENDER,\n",
    "    SegmentationSchema.EMPLOYEE_JOB_TITLE,\n",
    "]\n",
    "\n",
    "# Prepare Segmentation dataframes\n",
    "archetype_dataframe = ingestion_pipeline.dataframes[IngestionPipelineStages.SEGMENTATION_STAGE].copy()\n",
    "segmentation_dataframe = archetype_dataframe.copy()[segmentation_columns]\n",
    "demographic_dataframe = archetype_dataframe.copy()[demographic_columns]\n",
    "\n",
    "segmenter = DataSegmenter(\n",
    "    env=env,\n",
    "    pipeline_config=ingestion_pipeline.to_dict(),\n",
    "    segmentation_dataframe=segmentation_dataframe,\n",
    "    demographic_dataframe=demographic_dataframe,\n",
    "    categorical_columns=categorical_columns\n",
    ")\n",
    "\n",
    "unique_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "segmenter.segmenter_id  = f\"Data Segmentation: {unique_id}\"\n",
    "segmenter.segment_sequence(scaling_method=\"standard\", n_clusters=6)\n",
    "\n",
    "for feature in segmenter.final_dataframe.columns:\n",
    "    archetype_dataframe[feature] = segmenter.final_dataframe[feature]\n",
    "\n",
    "with open('src/mappings/ARCHETYPE_mapping.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data = {float(key): value for key, value in data.items()}\n",
    "archetype_dataframe['ARCHETYPE'] = archetype_dataframe['CLUSTER_ID'].map(data)\n",
    "\n",
    "archetype_dataframe.to_csv(path.join(env.visual_dir, \"final_segmentation.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0ae87",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.machine_learning.trainer import train\n",
    "\n",
    "train(\n",
    "    ingestion_pipeline=ingestion_pipeline,\n",
    "    config=conf,\n",
    "    environment=env,\n",
    "    limit=0.0005,\n",
    "    use_k_fold=False,\n",
    "    accelerator=\"gpu\",\n",
    "    use_sweep=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ba5f",
   "metadata": {},
   "source": [
    "## Hyperparameters sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb import sweep\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"test/auroc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"cutoff\": {\"values\": [0.3, 0.4, 0.5]},\n",
    "        \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 0.001, \"max\": 0.1},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = sweep(sweep_config, project=\"alayacare\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.machine_learning.sweep import run_sweep\n",
    "from src.machine_learning.trainer import train\n",
    "# Run the hyperparameter sweeps for the training loop\n",
    "\n",
    "run_sweep(\n",
    "    sweep_id=sweep_id,\n",
    "    train=lambda : train(\n",
    "        ingestion_pipeline=ingestion_pipeline,\n",
    "        config=conf,\n",
    "        environment=env,\n",
    "        limit=0.1,\n",
    "        use_k_fold=False,\n",
    "        use_sweep=True,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668e6d3",
   "metadata": {},
   "source": [
    "## Explaining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from pandas import DataFrame\n",
    "from torch import load\n",
    "from numpy import int64\n",
    "from src.machine_learning.model.perceptron import Perceptron\n",
    "from src.machine_learning.data.module.alayacare_data_module import AlayaCareDataModule\n",
    "from src.analysis.shap.shap_explainer import SHAPExplainer\n",
    "from src.utility.logger import Logger\n",
    "from src.utility.configs.explanation_config import ExplanationConfig\n",
    "\n",
    "explanation_config = ExplanationConfig(\n",
    "    nsamples=3,\n",
    "    sample_size=250,\n",
    "    seed=hash(\"alayacare\") % 2**32 - 1,\n",
    ")\n",
    "\n",
    "logger = Logger(env=env)\n",
    "logger.wandb.login(key=env.wandb_api_key)\n",
    "run = logger.wandb.init(\n",
    "    project=\"alayacare\",\n",
    "    group=\"explainability\",\n",
    ")\n",
    "artifact = run.use_artifact('alayacare/alayacare/model.ckpt:explainability', type='model', aliases=[\"model\"], use_as=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "datamodule = AlayaCareDataModule(\n",
    "    ingestion_pipeline=ingestion_pipeline,\n",
    "    config=conf,\n",
    ").prepare_data().setup(\"setup_splits\", fold_number=0)\n",
    "\n",
    "model = Perceptron(\n",
    "        model_config=conf,\n",
    "        in_features=datamodule.n_features,\n",
    "        out_features=datamodule.n_classes,\n",
    "        gender_embedding_size=datamodule.n_genders,\n",
    "        state_embedding_size=datamodule.n_states,\n",
    "        example_input_array=datamodule.dataset[0][0],\n",
    "        environment=env,\n",
    "    )\n",
    "model.load_state_dict(load(path.join(str(artifact_dir), 'model_weights.pt')))\n",
    "model.eval()\n",
    "\n",
    "xs, ys = next(iter(datamodule.test_dataloader()))\n",
    "test_dataframe = DataFrame(xs.numpy(), columns=datamodule.dataset.dataframe.columns[2:-1])\n",
    "\n",
    "shap_explainer = SHAPExplainer(\n",
    "    model=model,\n",
    "    dataframe=test_dataframe,\n",
    "    nsamples=explanation_config.nsamples,\n",
    "    seed=explanation_config.seed,\n",
    ")\n",
    "explanation = shap_explainer.explain(test_dataframe[:explanation_config.sample_size])\n",
    "\n",
    "\n",
    "logger.log_plot(\"explanation\", \"beeswarm\", explanation.bee_swarm_plot())\n",
    "logger.log_plot(\"explanation\", \"waterfall\", explanation.waterfall_plot(prediction_index=0))\n",
    "for feature_name in explanation.feature_names:\n",
    "    logger.log_plot(\"explanation_dependency\", feature_name, explanation.dependency_scatter_plot(feature_name=feature_name))\n",
    "logger.log_plot(\"explanation\", \"decision\", explanation.decision_plot())\n",
    "logger.log_plot(\n",
    "    \"explanation\",\n",
    "    \"misclassified_decision\",\n",
    "    explanation.decision_plot_misclassified(config=conf, y_true=ys[:explanation_config.sample_size].numpy().astype(int64))\n",
    ")\n",
    "\n",
    "logger.wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3711d64",
   "metadata": {},
   "source": [
    "## Volume Test for Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(\"src\")\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "from pandas import Timestamp\n",
    "from src.utility.environment import Environment\n",
    "from src.utility.configs.config import Config\n",
    "from src.data.ingestion_pipeline.ingestion_pipeline import IngestionPipeline\n",
    "from src.data.ingestion_pipeline.ingestion_pipeline_stages import IngestionPipelineStages\n",
    "from src.data.ingestion_pipeline.stages.cleaning.client_data_cleaning_calculated_fields_stage import build_client_data_cleaning_calculated_fields_stage\n",
    "from src.data.ingestion_pipeline.stages.cleaning.clock_data_cleaning_calculated_fields_stage import build_clock_data_cleaning_calculated_fields_stage\n",
    "from src.data.ingestion_pipeline.stages.cleaning.employee_data_cleaning_calculated_fields_stage import build_employee_data_cleaning_calculated_fields_stage\n",
    "from src.data.ingestion_pipeline.stages.cleaning.visit_data_cleaning_calculated_fields_stage import build_visit_data_cleaning_calculated_fields_stage\n",
    "from src.data.ingestion_pipeline.stages.augmented_visit_stage import build_augmented_visit_stage\n",
    "from src.data.ingestion_pipeline.stages.employee_history_aggregation_stage import build_employee_history_aggregation_stage\n",
    "from src.data.ingestion_pipeline.stages.employee_history_fill_gaps_stage import build_employee_history_fill_gaps_stage\n",
    "from src.data.ingestion_pipeline.stages.employee_history_calculated_fields_stage import build_employee_history_calculated_fields_stage\n",
    "from src.data.ingestion_pipeline.stages.employee_history_fill_na_stage import build_employee_history_fill_na_stage\n",
    "from src.data.ingestion_pipeline.stages.employee_history_anomaly_detection_stage import build_employee_history_anomaly_detection_stage\n",
    "from src.data.ingestion_pipeline.stages.segmentation_stage import build_segmentation_stage\n",
    "from src.data.ingestion_pipeline.stages.training_employee_history_stage import build_training_employee_history_stage\n",
    "from src.data.ingestion_pipeline.stages.y_labels_generation_stage import build_y_labels_generation_stage\n",
    "\n",
    "# 6 Hours\n",
    "MAX_RUNTIME = 21600\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "env = Environment()\n",
    "conf = Config(\n",
    "    load_id=datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    n_splits=2,\n",
    "    split_seed=5832391,\n",
    "    log_every_n_steps=50,\n",
    "    training_window_size=1,\n",
    "    n_epochs=5,\n",
    "    batch_size=16384,\n",
    "    label_policy=\"90Days\",\n",
    "    period_duration=\"1D\",\n",
    "    period_start=Timestamp(year=2019, month=1, day=1),\n",
    "    period_end=Timestamp(year=2023, month=12, day=31),\n",
    "    cutoff=0.5,\n",
    "    oversampler=\"SMOTE\",\n",
    "    oversampler_args={},\n",
    "    model=\"ExplainableBoostingMachine\",\n",
    "    model_config={},\n",
    "    is_test_run=True\n",
    ")\n",
    "\n",
    "ingestion_pipeline = IngestionPipeline(\n",
    "    config=conf,\n",
    "    environment=env,\n",
    "    use_caching=False,\n",
    "    stages=[\n",
    "        build_client_data_cleaning_calculated_fields_stage(conf, env),\n",
    "        build_clock_data_cleaning_calculated_fields_stage(conf, env),\n",
    "        build_employee_data_cleaning_calculated_fields_stage(conf, env),\n",
    "        build_visit_data_cleaning_calculated_fields_stage(conf, env),\n",
    "        build_augmented_visit_stage(conf, env),\n",
    "        build_employee_history_aggregation_stage(conf, env),\n",
    "        build_employee_history_fill_gaps_stage(conf, env),\n",
    "        build_employee_history_calculated_fields_stage(conf, env),\n",
    "        build_employee_history_fill_na_stage(conf, env),\n",
    "        build_employee_history_anomaly_detection_stage(conf, env),\n",
    "        build_y_labels_generation_stage(conf, env),\n",
    "        build_segmentation_stage(conf, env),\n",
    "        build_training_employee_history_stage(conf, env),\n",
    "    ],\n",
    ").build_pipeline().run_pipeline()\n",
    "\n",
    "end = datetime.now()\n",
    "elapsed_time = (end - start)\n",
    "\n",
    "assert elapsed_time < MAX_RUNTIME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1c4cd",
   "metadata": {},
   "source": [
    "## Inference & Visualization Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b74a1",
   "metadata": {},
   "source": [
    "### Preparing Inference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.ingestion_pipeline.ingestion_pipeline_stages import IngestionPipelineStages\n",
    "from src.data.schema.employee_history_schema import EmployeeHistorySchema\n",
    "from src.data.schema.employee_schema import EmployeeSchema\n",
    "from pandas import to_timedelta\n",
    "\n",
    "DAYS_IN_YEAR = 365\n",
    "\n",
    "# Retrieve the employee dataframe\n",
    "employee_df = ingestion_pipeline.dataframes[IngestionPipelineStages.EMPLOYEE_DATA_CLEANING_CALCULATED_FIELDS_STAGE]\n",
    "\n",
    "# Retain employees with EMPLOYEE_STATUS 1 (active) or 3 (suspended)\n",
    "filtered_employee_df = employee_df #[employee_df[EmployeeSchema.EMPLOYEE_STATUS].isin([1, 3])]\n",
    "\n",
    "# Retrieve the employee history dataframe\n",
    "employee_history_df = ingestion_pipeline.dataframes[IngestionPipelineStages.EMPLOYEE_HISTORY_AGGREGATION_STAGE]\n",
    "\n",
    "# filter the employee history dataframe to only keep employees without a TERMINATION_DATE\n",
    "non_terminated_employees_list = employee_history_df[employee_history_df[EmployeeHistorySchema.EMPLOYEE_TERMINATION_DATE].isnull()][EmployeeHistorySchema.EMPLOYEE_ID].unique()\n",
    "\n",
    "# Employees with EMPLOYEE_STATUS 1 (active) or 2 (suspended) and without a TERMINATION_DATE\n",
    "current_employees_df = filtered_employee_df[filtered_employee_df[EmployeeSchema.EMPLOYEE_ID].isin(non_terminated_employees_list)]\n",
    "\n",
    "# Retrieve the training dataframe\n",
    "training_df = ingestion_pipeline.dataframes[IngestionPipelineStages.TRAINING_EMPLOYEE_HISTORY_STAGE]\n",
    "\n",
    "# Filter the training dataframe to only keep current employees\n",
    "current_training_df = training_df[training_df[EmployeeHistorySchema.EMPLOYEE_ID].isin(current_employees_df[EmployeeSchema.EMPLOYEE_ID])]\n",
    "\n",
    "# Further filter the training dataframe to only keep the latest period for each current employee\n",
    "current_training_latest_period_df = current_training_df.loc[current_training_df.groupby(EmployeeHistorySchema.EMPLOYEE_ID)[EmployeeHistorySchema.PERIOD_START].idxmax()].reset_index()\n",
    "\n",
    "# Employees that have started before the last year AND have not quit before the last year\n",
    "yoy_employees_list = employee_history_df[\n",
    "    ((employee_history_df[EmployeeHistorySchema.EMPLOYEE_TERMINATION_DATE].isnull()) |\n",
    "    (employee_history_df[EmployeeHistorySchema.EMPLOYEE_TERMINATION_DATE] > conf.period_end - to_timedelta(DAYS_IN_YEAR, unit='D'))) &\n",
    "    (employee_history_df[EmployeeHistorySchema.EMPLOYEE_START_ON] < conf.period_end - to_timedelta(DAYS_IN_YEAR, unit='D'))\n",
    "    ][EmployeeHistorySchema.EMPLOYEE_ID].unique()\n",
    "\n",
    "# Filter the training df to keep YOY data\n",
    "yoy_training_df = training_df[training_df[EmployeeHistorySchema.EMPLOYEE_ID].isin(yoy_employees_list)]\n",
    "\n",
    "# Further filter the yoy_training_df to only keep the latest period for each yoy employee\n",
    "yoy_training_latest_period_df = yoy_training_df.loc[yoy_training_df.groupby(EmployeeHistorySchema.EMPLOYEE_ID)[EmployeeHistorySchema.PERIOD_START].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98427a84",
   "metadata": {},
   "source": [
    "### Inference with an sk-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pickle\n",
    "from interpret import show\n",
    "\n",
    "from pandas import DataFrame\n",
    "from numpy import ndarray, select\n",
    "\n",
    "# Update model file path according to desired model for inference\n",
    "MODEL_FILE_PATH = path.join(env.model_dir,'ExplainableBoostingMachine' ,'model_00000.pkl')\n",
    "\n",
    "# Update the thresholds according to SME's recommendations\n",
    "HIGH_RISK_THRESHOLD = 0.75\n",
    "MEDIUM_RISK_THRESHOLD = 0.5\n",
    "\n",
    "# Retrieve selected churn prediction model\n",
    "with open(MODEL_FILE_PATH, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "def run_predictions(df: DataFrame, model):\n",
    "    \"\"\"Runs inference on a dataframe and stores prediction probabilities in new column \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): the input DataFrame\n",
    "        model (): the churn prediction model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: the input DataFrame updated with a CHURN_PROBABILITY column \n",
    "    \"\"\"\n",
    "    predictions = model.predict_proba(df)\n",
    "\n",
    "    if isinstance(predictions, ndarray) and predictions.shape[1] == 2:\n",
    "                predictions = predictions[:, 1]\n",
    "\n",
    "    df['CHURN_PROBABILITY'] = predictions\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_churn_risk_column(df):\n",
    "    \"\"\"Bins CHURN_PROBABILITY into a textual CHURN_RISK column \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): the input DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: the input DataFrame updated with a CHURN_RISK column\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        (df['CHURN_PROBABILITY'] <= MEDIUM_RISK_THRESHOLD),\n",
    "        (MEDIUM_RISK_THRESHOLD < df['CHURN_PROBABILITY']) & (df['CHURN_PROBABILITY'] <= HIGH_RISK_THRESHOLD),\n",
    "        (df['CHURN_PROBABILITY'] > HIGH_RISK_THRESHOLD)\n",
    "    ]\n",
    "\n",
    "    values = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "\n",
    "    df['CHURN_RISK'] = select(conditions, values)\n",
    "\n",
    "    return df\n",
    "\n",
    "current_visual_dataset = run_predictions(current_training_latest_period_df, model)\n",
    "current_visual_dataset = create_churn_risk_column(current_visual_dataset)\n",
    "\n",
    "yoy_visual_dataset = run_predictions(yoy_training_latest_period_df, model)\n",
    "yoy_visual_dataset = create_churn_risk_column(yoy_visual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Feature Importance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, melt\n",
    "\n",
    "local_explanations = model.explain_local(current_training_latest_period_df[model.feature_names].values)\n",
    "\n",
    "explanation_columns =  local_explanations.data(0)['names']\n",
    "\n",
    "explanations_df = DataFrame(columns=explanation_columns)\n",
    "\n",
    "for i in range(len(current_training_latest_period_df)):\n",
    "    score = local_explanations.data(i)['scores']\n",
    "    explanations_df.loc[len(explanations_df)] = score\n",
    "\n",
    "explanations_df['EMPLOYEE_ID'] = current_training_latest_period_df['EMPLOYEE_ID']\n",
    "\n",
    "explanations_df = melt(explanations_df, id_vars=['EMPLOYEE_ID'], var_name='FEATURE_NAME', value_name='FEATURE_IMPORTANCE')\n",
    "\n",
    "explanations_df.to_csv(path.join(env.visual_dir, f\"explanations_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bebf0",
   "metadata": {},
   "source": [
    "### Preparing the dataset for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9639ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import merge\n",
    "\n",
    "from src.utility.mapping_loader import load_json_mapping_to_dict\n",
    "\n",
    "EMPLOYEE_GENDER_MAPPING_PATH = \"src/mappings/EMPLOYEE_GENDER_mapping.json\"\n",
    "EMPLOYEE_STATE_MAPPING_PATH = \"src/mappings/EMPLOYEE_STATE_mapping.json\"\n",
    "EMPLOYEE_STATUS_MAPPING_PATH = \"src/mappings/employee_status_mapping.json\"\n",
    "JOB_TITLE_MAPPING_PATH = \"src/mappings/EMPLOYEE_JOB_TITLE_mapping.json\"\n",
    "\n",
    "def prepare_visual_dataset(df):\n",
    "    # Retrieve employee dataframe\n",
    "    employee_df = ingestion_pipeline.dataframes[IngestionPipelineStages.EMPLOYEE_DATA_CLEANING_CALCULATED_FIELDS_STAGE]\n",
    "\n",
    "    # Drop columns from visual_dataset that will be pulled from employee_df\n",
    "    columns_to_keep = ['EMPLOYEE_ID'] + [col for col in df.columns if col not in employee_df.columns]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Merge visual_dataset and employee_df\n",
    "    df = merge(df, employee_df, on=EmployeeHistorySchema.EMPLOYEE_ID)\n",
    "\n",
    "    # Replace EMPLOYEE_GENDER with text\n",
    "    employee_gender_mapping = load_json_mapping_to_dict(EMPLOYEE_GENDER_MAPPING_PATH)\n",
    "    df[EmployeeSchema.EMPLOYEE_GENDER] = df[EmployeeSchema.EMPLOYEE_GENDER].map(employee_gender_mapping).fillna(\"Unknown\")\n",
    "\n",
    "    # Replace EMPLOYEE_STATE with text\n",
    "    employee_state_mapping = load_json_mapping_to_dict(EMPLOYEE_STATE_MAPPING_PATH)\n",
    "    df[EmployeeSchema.EMPLOYEE_STATE] = df[EmployeeSchema.EMPLOYEE_STATE].map(employee_state_mapping).fillna(\"Unknown\")\n",
    "\n",
    "    # Replace EMPLOYEE_STATUS with text\n",
    "    employee_status_mapping = load_json_mapping_to_dict(EMPLOYEE_STATUS_MAPPING_PATH)\n",
    "    df[EmployeeSchema.EMPLOYEE_STATUS] = df[EmployeeSchema.EMPLOYEE_STATUS].map(employee_status_mapping).fillna(\"Unknown\")\n",
    "\n",
    "    # Replace JOB_DESCRIPTION with text\n",
    "    job_description_mapping = load_json_mapping_to_dict(JOB_TITLE_MAPPING_PATH)\n",
    "    df[EmployeeSchema.EMPLOYEE_JOB_TITLE] = df[EmployeeSchema.EMPLOYEE_JOB_TITLE].map(job_description_mapping).fillna(\"Unknown\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Prepare and save visual datasets\n",
    "current_visual_dataset = prepare_visual_dataset(current_visual_dataset)\n",
    "current_visual_dataset.to_csv(path.join(env.visual_dir, f\"current_visual_dataset.csv\"), index=False)\n",
    "\n",
    "yoy_visual_dataset = prepare_visual_dataset(yoy_visual_dataset)\n",
    "yoy_visual_dataset.to_csv(path.join(env.visual_dir, f\"yoy_visual_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset for the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from pandas import DataFrame\n",
    "from src.data.schema.augmented_visit_schema import AugmentedVisitSchema\n",
    "\n",
    "MAP_DATASET_DAY_SPAN = 365\n",
    "\n",
    "# Retrieve the relevant columns for the map dataset from AUGMENT_VISIT_STAGE\n",
    "map_columns = [AugmentedVisitSchema.EMPLOYEE_ID, AugmentedVisitSchema.VISIT_START_AT, AugmentedVisitSchema.CLIENT_LATITUDE, AugmentedVisitSchema.CLIENT_LONGITUDE]\n",
    "map_df = ingestion_pipeline.dataframes[IngestionPipelineStages.AUGMENT_VISIT_STAGE][map_columns]\n",
    "\n",
    "# Filter out employees that are not in the visual_dataset\n",
    "map_df = map_df[map_df[AugmentedVisitSchema.EMPLOYEE_ID].isin(current_visual_dataset[AugmentedVisitSchema.EMPLOYEE_ID])]\n",
    "\n",
    "# Rename columns\n",
    "map_df = map_df.rename(columns={\n",
    "    AugmentedVisitSchema.VISIT_START_AT: 'DATE',\n",
    "    AugmentedVisitSchema.CLIENT_LATITUDE: 'LATITUDE',\n",
    "    AugmentedVisitSchema.CLIENT_LONGITUDE: 'LONGITUDE'\n",
    "})\n",
    "\n",
    "# Set COORDINATE_TYPE to client for all rows\n",
    "map_df[\"COORDINATE_TYPE\"] = \"CLIENT\"\n",
    "\n",
    "# Append rows with the coordinates of each employee \n",
    "for employee_id in map_df[AugmentedVisitSchema.EMPLOYEE_ID].unique():\n",
    "    # Filter employee_df for the specific EMPLOYEE_ID\n",
    "    employee_data = employee_df[employee_df[EmployeeSchema.EMPLOYEE_ID] == employee_id]\n",
    "\n",
    "    if not employee_data.empty:\n",
    "        employee_latitude = employee_data.iloc[0][EmployeeSchema.EMPLOYEE_LATITUDE] + 0.005 # Adding noise to avoid overlap of employee and client bubbles on the map\n",
    "        employee_longitude = employee_data.iloc[0][EmployeeSchema.EMPLOYEE_LONGITUDE]\n",
    "\n",
    "        # Create a new row with EMPLOYEE_ID and LATITUDE\n",
    "        new_row = DataFrame({\n",
    "            AugmentedVisitSchema.EMPLOYEE_ID: [employee_id],\n",
    "            'DATE': [conf.period_end],\n",
    "            'LATITUDE': [employee_latitude], \n",
    "            'LONGITUDE': [employee_longitude],\n",
    "            'COORDINATE_TYPE': 'EMPLOYEE'\n",
    "        })\n",
    "\n",
    "        # Append the new row to map_df\n",
    "        map_df = map_df.append(new_row, ignore_index=True)\n",
    "\n",
    "map_df\n",
    "\n",
    "# Save map dataset to csv\n",
    "map_df.to_csv(path.join(env.visual_dir, f\"map_dataset.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
